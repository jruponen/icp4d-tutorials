{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Mortgage Default in Loan Marketplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will learn how to build a predictive model with Spark machine learning API (SparkML) and deploy it for scoring in Machine Learning (ML).\n",
    "\n",
    "This notebook walks you through these steps:\n",
    "\n",
    "    - Build a model with SparkML API\n",
    "    - Save the model in the ML repository\n",
    "    - Create a Deployment in ML (via UI)\n",
    "    - Test the model (via UI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Python version. Make sure it is Python 3.6.x. \n",
    "import platform\n",
    "print(platform.python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall the older Watson Machine Learning client \n",
    "!pip uninstall watson-machine-learning-client -y\n",
    "\n",
    "# Install the WML client \n",
    "!pip install watson-machine-learning-client-V4\n",
    "\n",
    "# Verify WLM Client version\n",
    "!pip list | grep watson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install findspark\n",
    "!pip install pyspark==2.3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action: restart the kernel!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Authenticate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WML_CREDENTIALS = {\n",
    "   \"instance_id\": \"openshift\",\n",
    "   \"url\" : \"https://zen-cpd-zen.apps.testcluster.demo.ibmcloud.com\",\n",
    "   \"username\":\"admin\",\n",
    "   \"password\": \"passw0rd\",\n",
    "   \"version\": \"2.5.0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter the values for you database connection found under data virtualization \n",
    "dsn_url = \"jdbc:db2://dv-server.zen.svc.cluster.local:32051/bigsql\"   # e.g. \"jdbc:db2://dv-server.zen.local:32051/bigsql\"\n",
    "dsn_uid = \"user1022\"                                                  # e.g. \"user999\"\n",
    "dsn_pwd = \"sw?#@lT_674MfPI5\"                                          # e.g. \"7dBZ3jWt9xN6$o0JiX!m\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load data  --- Update the *dsn* values with your Data Virtualization credential "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaydebeapi, sys\n",
    "import pandas as pd\n",
    "import findspark\n",
    "import pyspark\n",
    "import json\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string=dsn_url\n",
    "if (sys.version_info >= (3,0)):\n",
    "    conn = jaydebeapi.connect(\"com.ibm.db2.jcc.DB2Driver\", connection_string, [dsn_uid, dsn_pwd])\n",
    "else:\n",
    "    conn = jaydebeapi.connect(\"com.ibm.db2.jcc.DB2Driver\", [connection_string, dsn_uid, dsn_pwd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.read_sql(\"select * from USER999.MORTGAGE_JOIN_VIEW\", con=conn)\n",
    "pdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sdf = spark.createDataFrame(pdf)\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build the Spark pipeline and the Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test datasets\n",
    "train, test = sdf.randomSplit([0.8,0.2], seed=11)\n",
    "\n",
    "MODEL_NAME = \"MORTGAGE PREDICTION MODEL\"\n",
    "DEPLOYMENT_NAME = \"MORTGAGE PREDICTION\"\n",
    "\n",
    "print(\"Number of records for training: \" + str(train.count()))\n",
    "print(\"Number of records for evaluation: \" + str(test.count()))\n",
    "\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates a Random Forest Classifier with Spark, setting up string indexers for the categorical features and the label column. Finally, this notebook creates a pipeline including the indexers and the model, and does an initial Area Under ROC evaluation of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline, Model\n",
    "\n",
    "# Prepare string variables so that they can be used by the decision tree algorithm\n",
    "# StringIndexer encodes a string column of labels to a column of label indices\n",
    "SI1 = StringIndexer(inputCol='APPLIED_ONLINE',outputCol='AppliedOnlineEncoded')\n",
    "SI2 = StringIndexer(inputCol='RESIDENCE',outputCol='ResidenceEncoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol='MORTGAGE_DEFAULT', outputCol='label').fit(sdf)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)\n",
    "\n",
    "# Pipelines API requires that input variables are passed in  a vector\n",
    "assembler = VectorAssembler(inputCols=[\"AppliedOnlineEncoded\", \"ResidenceEncoded\", \"INCOME\", \"YRS_CURRENT_ADD\", \\\n",
    "                                       \"YRS_CURRENT_EMP\", \"NO_OF_CARDS\", \"CARD_DEBT\", \"CURRENT_LOANS\", \"LOAN_AMOUNT\", \\\n",
    "                                       \"SALE_PRICE\", \"LOCATION\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "# instantiate the algorithm, take the default settings\n",
    "\n",
    "#rf=RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "rf=RandomForestClassifier(featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[SI1,SI2,labelIndexer, assembler, rf, labelConverter])\n",
    "\n",
    "# Build models\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test)\n",
    "evaluatorDT = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",  metricName='areaUnderROC')\n",
    "area_under_curve = evaluatorDT.evaluate(predictions)\n",
    "\n",
    "evaluatorDT = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",  metricName='areaUnderPR')\n",
    "area_under_PR = evaluatorDT.evaluate(predictions)\n",
    "\n",
    "#default evaluation is areaUnderROC\n",
    "print(\"areaUnderROC = %g\" % area_under_curve, \"areaUnderPR = %g\" % area_under_PR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Score the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.transform(test)\n",
    "results=results.select(results[\"MORTGAGE_DEFAULT\"],results[\"label\"],results[\"predictedLabel\"],results[\"prediction\"],results[\"probability\"])\n",
    "results.toPandas().head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=results.filter(results.label == results.prediction).count() / float(results.count())\n",
    "print('Accuracy = {:.2f}.'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "print('Area under ROC curve = {:.2f}.'.format(evaluator.evaluate(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Set default space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "import json\n",
    "client = WatsonMachineLearningAPIClient(WML_CREDENTIALS)\n",
    "client.repository.list_spaces()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action: Find out the GUID for space name 'MortgageDeploymentSpace' from above output.\n",
    "###              In the following cell replace the GUID with one that you found above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: client.set.default_space('b49e13e8-ec68-408d-84a1-957e28c154b1')\n",
    "client.set.default_space('GUID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Remove existing model and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_details = client.deployments.get_details()\n",
    "for deployment in deployment_details['resources']:\n",
    "    deployment_id = deployment['metadata']['guid']\n",
    "    model_id = deployment['entity']['asset']['href'].split('/')[3].split('?')[0]\n",
    "    if deployment['entity']['name'] == DEPLOYMENT_NAME:\n",
    "        print('Deleting deployment id', deployment_id)\n",
    "        client.deployments.delete(deployment_id)\n",
    "        print('Deleting model id', model_id)\n",
    "        client.repository.delete(model_id)\n",
    "       \n",
    "client.repository.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Save Model in ML repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_models = client.repository.get_model_details()\n",
    "model_id = None\n",
    "\n",
    "for model_in in wml_models['resources']:\n",
    "    if MODEL_NAME == model_in['entity']['name']:\n",
    "        model_id = model_in['metadata']['guid']\n",
    "        break\n",
    "\n",
    "if model_id is None:\n",
    "    print(\"Storing model ...\")\n",
    "    meta_props = {\n",
    "        client.repository.ModelMetaNames.NAME: MODEL_NAME,\n",
    "        client.repository.ModelMetaNames.TYPE: \"mllib_2.3\",\n",
    "        client.repository.ModelMetaNames.RUNTIME_UID: \"spark-mllib_2.3\"\n",
    "    }\n",
    "\n",
    "    model_artifact = client.repository.store_model( model=model,\n",
    "                                                pipeline=pipeline,\n",
    "                                                meta_props=meta_props,\n",
    "                                                training_data=train\n",
    "                                                )\n",
    "    model_id = client.repository.get_model_uid(model_artifact)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Create Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_deployments = client.deployments.get_details()\n",
    "deployment_uid = None\n",
    "for deployment in wml_deployments['resources']:\n",
    "    if DEPLOYMENT_NAME == deployment['entity']['name']:\n",
    "        deployment_uid = deployment['metadata']['guid']\n",
    "        break\n",
    "\n",
    "if deployment_uid is None:\n",
    "    print(\"Deploying model...\")        \n",
    "    meta_props = {\n",
    "        client.deployments.ConfigurationMetaNames.NAME: DEPLOYMENT_NAME,\n",
    "        client.deployments.ConfigurationMetaNames.ONLINE: {}\n",
    "    }\n",
    "    deployment = client.deployments.create(artifact_uid=model_id, meta_props=meta_props)\n",
    "    deployment_uid = client.deployments.get_uid(deployment)\n",
    "\n",
    "print(\"Model id: {}\".format(model_id))\n",
    "print(\"Deployment id: {}\".format(deployment_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the test data to a .csv so that we can later use it for Evaluation\n",
    "writeCSV=test.toPandas()\n",
    "writeCSV.to_csv('./MortgagePredictionModelEval.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Test Saved Model with Test UI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Save the notebook \n",
    "2. Test the deployed module from Analyze > Analytics deployment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have finished working on this hands-on lab. In this notebook you created a model using SparkML API, deployed it in Machine Learning service for online (real time) scoring and tested it using a test client.\n",
    "\n",
    "Created by Sanjit Chakraborty sanjitc@us.ibm.com Nov 8, 2019."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
